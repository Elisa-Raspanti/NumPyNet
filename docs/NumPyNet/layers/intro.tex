
Layers are the main core of a Convolutional Neural Network: every one of them performs, on its respective input, a different operation.
The concatenation of multiple Layer form a CNN, where for concatenation we mean that the output of a layer become the input of the next one.

In the NumPyNet framework a Layer is a python \lstinline{class}, this allow the users to instantiate an object of the chosen type and call one of its methods.

Main Method:
\begin{itemize}
 \title{Main Methods}
 \item \textbf{forward}  : this function is defined for every Layer and perform the so-called \textit{forward pass}, that is the implementation of the transformation the Layer performs on the Input.
    It usually receives as argument just the output of the previous Layer
 \item \textbf{backward} : this function is defined for every Layer and perform the so-called \textit{backward pass}, that is an implementation of the BackPropagation algorithm for the error.
    It computes the delta to be back-propagated during \textit{training} and, eventually, the updates to trainable weights
    It usually receives as input only the global delta of the network, on which it performs a transformation, depending on the layer.
 \item \textbf{update}   : this function is defined only for layers with trainable weights, updating them following a Gradient Descent with Momentum algorithm.
    It receives as parameters:
    \begin{itemize}
     \item momentum  :float, default = 0., scale factor of weight update
     \item decay     :float, default = 0., determines the decay of weights\_update
     \item lr        :float, default = 1e-02, learning rate of the layer
     \item lr\_scale :float, default = 1., learning rate scale of the layer
    \end{itemize}
\end{itemize}



 
 
